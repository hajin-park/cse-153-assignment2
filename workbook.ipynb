{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca02b63b",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Conditioned drum beat prediction on 2-d pose data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272af5b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2411934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hajin\\miniconda3\\envs\\cse-153-assignment2\\Lib\\site-packages\\pretty_midi\\instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab41c",
   "metadata": {},
   "source": [
    "### Model training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd553e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT_SEC = 0.02 # beat time resolution\n",
    "DRUM_TOKENS: Dict[str, int] = {\n",
    "    \"pad\": 0,\n",
    "    \"kick\": 1,\n",
    "    \"snare\": 2,\n",
    "    \"hihat_closed\": 3,\n",
    "    \"hihat_open\": 4,\n",
    "    \"tom_low\": 5,\n",
    "    \"tom_mid\": 6,\n",
    "    \"tom_high\": 7,\n",
    "    \"crash\": 8,\n",
    "    \"ride\": 9,\n",
    "}\n",
    "# time‑shift tokens (20 ms each, up to 2 s)\n",
    "SHIFT_OFFSET = len(DRUM_TOKENS)\n",
    "MAX_SHIFT = 100  # 100 × 20 ms  = 2 s\n",
    "for i in range(1, MAX_SHIFT + 1):\n",
    "    DRUM_TOKENS[f\"shift_{i}\"] = SHIFT_OFFSET + i\n",
    "\n",
    "# sequence control tokens\n",
    "DRUM_TOKENS[\"bos\"] = len(DRUM_TOKENS)  # begin‑of‑sequence\n",
    "DRUM_TOKENS[\"eos\"] = len(DRUM_TOKENS)  # end‑of‑sequence\n",
    "\n",
    "VOCAB_SIZE = len(DRUM_TOKENS)\n",
    "IDX2TOKEN = {v: k for k, v in DRUM_TOKENS.items()}\n",
    "PAD_IDX = DRUM_TOKENS[\"pad\"]\n",
    "BOS_IDX = DRUM_TOKENS[\"bos\"]\n",
    "EOS_IDX = DRUM_TOKENS[\"eos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a67c22",
   "metadata": {},
   "source": [
    "### Model Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715e78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pitch_to_token(p: int) -> str:\n",
    "    # General MIDI → symbolic token\n",
    "    return (\n",
    "        \"kick\"\n",
    "        if p in (35, 36)\n",
    "        else (\n",
    "            \"snare\"\n",
    "            if p in (38, 40)\n",
    "            else (\n",
    "                \"hihat_closed\"\n",
    "                if p in (42, 44)\n",
    "                else (\n",
    "                    \"hihat_open\"\n",
    "                    if p == 46\n",
    "                    else (\n",
    "                        \"tom_low\"\n",
    "                        if p in (41, 45)\n",
    "                        else (\n",
    "                            \"tom_mid\"\n",
    "                            if p in (47, 48)\n",
    "                            else (\n",
    "                                \"tom_high\"\n",
    "                                if p == 50\n",
    "                                else (\n",
    "                                    \"crash\"\n",
    "                                    if p in (49, 57)\n",
    "                                    else \"ride\" if p in (51, 59) else \"snare\"\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47fc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_tokens(mid: pm.PrettyMIDI, time_unit: float = SHIFT_SEC) -> List[int]:\n",
    "    \"\"\"Drum MIDI -> event tokens (no BOS/EOS).\"\"\"\n",
    "    events: List[Tuple[float, str]] = []\n",
    "    for inst in mid.instruments:\n",
    "        if not inst.is_drum:\n",
    "            continue\n",
    "        for note in inst.notes:\n",
    "            events.append((note.start, _pitch_to_token(note.pitch)))\n",
    "    events.sort(key=lambda x: x[0])\n",
    "\n",
    "    tokens, prev_time = [], 0.0\n",
    "    for t, tok in events:\n",
    "        delta = t - prev_time\n",
    "        n_shift = int(round(delta / time_unit))\n",
    "        while n_shift > MAX_SHIFT:\n",
    "            tokens.append(DRUM_TOKENS[f\"shift_{MAX_SHIFT}\"])\n",
    "            n_shift -= MAX_SHIFT\n",
    "        if n_shift > 0:\n",
    "            tokens.append(DRUM_TOKENS[f\"shift_{n_shift}\"])\n",
    "        tokens.append(DRUM_TOKENS[tok])\n",
    "        prev_time = t\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c221fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pose, tok = zip(*batch)\n",
    "    return (torch.nn.utils.rnn.pad_sequence(pose, batch_first=True), torch.stack(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21b319",
   "metadata": {},
   "source": [
    "### Model dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b780764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoreoGrooveDataset(Dataset):\n",
    "    def __init__(self, root: str, seq_len: int = 512):\n",
    "        self.items = sorted(glob(os.path.join(root, \"*\", \"pose.npy\")))\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pose_path = self.items[idx]\n",
    "        drum_path = pose_path.replace(\"pose.npy\", \"drums.mid\")\n",
    "\n",
    "        # pose -> features\n",
    "        pose = np.load(pose_path).reshape(-1, 51)  # (T, 17x3)\n",
    "        vel = np.diff(pose, axis=0, prepend=pose[:1])\n",
    "        feats = np.concatenate([pose, vel], axis=-1)  # (T, 102)\n",
    "        feats = (feats - feats.mean()) / (feats.std() + 1e-5)\n",
    "        feats = feats.astype(np.float32)\n",
    "\n",
    "        # drums -> tokens  [+ BOS/EOS, pad/trim]\n",
    "        tokens = [BOS_IDX] + midi_to_tokens(pm.PrettyMIDI(drum_path)) + [EOS_IDX]\n",
    "        if len(tokens) < self.seq_len:\n",
    "            tokens += [PAD_IDX] * (self.seq_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[: self.seq_len]\n",
    "\n",
    "        return torch.from_numpy(feats), torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7411",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2868be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEncoder(nn.Module):\n",
    "    def __init__(self, in_feats=102, embed=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_feats, 128, 5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embed, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRU(embed, embed, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(embed * 2, embed)\n",
    "\n",
    "    def forward(self, x):  # x (B,T,F)\n",
    "        x = self.conv(x.transpose(1, 2)).transpose(1, 2)  # (B,T,E)\n",
    "        x, _ = self.gru(x)\n",
    "        return self.proj(x).transpose(0, 1)  # (T,B,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9234955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumDecoder(nn.Module):\n",
    "    def __init__(self, embed=256, layers=4, nhead=8, vocab=VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab, embed)\n",
    "        self.pos_emb = nn.Embedding(1024, embed)\n",
    "        dec_layer = nn.TransformerDecoderLayer(embed, nhead, 1024, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(dec_layer, layers)\n",
    "        self.fc_out = nn.Linear(embed, vocab)\n",
    "\n",
    "    def forward(self, tgt, memory):  # tgt (B,L), memory (T,B,E)\n",
    "        pos = torch.arange(tgt.size(1), device=tgt.device).unsqueeze(0)\n",
    "        tgt = self.tok_emb(tgt) + self.pos_emb(pos)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(\n",
    "            tgt.device\n",
    "        )\n",
    "        out = self.transformer(tgt, memory.transpose(0, 1), tgt_mask=mask)\n",
    "        return self.fc_out(out)  # (B,L,V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d87da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choreo2GrooveModel(pl.LightningModule):\n",
    "    def __init__(self, in_feats: int, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.encoder = PoseEncoder(in_feats)\n",
    "        self.decoder = DrumDecoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    # forward\n",
    "    def forward(self, poses, tokens):\n",
    "        memory = self.encoder(poses)  # (T,B,E)\n",
    "        if self.training:\n",
    "            tgt_in = tokens[:, :-1]  # strip last (EOS / PAD)\n",
    "            return self.decoder(tgt_in, memory)  # (B,L‑1,V)\n",
    "        else:\n",
    "            return self.decoder(tokens, memory)\n",
    "\n",
    "    # training\n",
    "    def training_step(self, batch, _):\n",
    "        pose, tok = batch\n",
    "        logits = self(pose, tok)\n",
    "        loss = self.loss_fn(logits.reshape(-1, VOCAB_SIZE), tok[:, 1:].reshape(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67b722",
   "metadata": {},
   "source": [
    "### Training Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62636b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "lr=1e-4\n",
    "batch_size=4\n",
    "seq_len=256\n",
    "version=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8491ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_availability():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"Using GPU\")\n",
    "        return True, gpu_count\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4febe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "has_gpu, gpu_count = check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef359a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DataLoader - optimized for GPU\n",
    "num_workers = (\n",
    "    0 if sys.platform.startswith(\"win\") else min(4, gpu_count * 2) if has_gpu else 2\n",
    ")\n",
    "pin_memory = has_gpu  # Use pinned memory for faster GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c99dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: increasing batch size from 4 to 8\n"
     ]
    }
   ],
   "source": [
    "# Adjust batch size for GPU if available\n",
    "if has_gpu and batch_size < 8:\n",
    "    original_batch_size = batch_size\n",
    "    batch_size = min(16, batch_size * 2)  # Increase batch size for GPU\n",
    "    print(\n",
    "        f\"GPU detected: increasing batch size from {original_batch_size} to {batch_size}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0d477",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca01b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChoreoGrooveDataset(\"dataset_root\", seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f57692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 76 samples, 102 features per frame\n"
     ]
    }
   ],
   "source": [
    "# Calculate input features from first sample\n",
    "sample_pose, _ = dataset[0]\n",
    "in_feats = sample_pose.shape[-1]\n",
    "print(f\"Dataset loaded: {len(dataset)} samples, {in_feats} features per frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c5a1e",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0d79073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Choreo2GrooveModel(in_feats=in_feats, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e10ab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184a6f6",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffaaf9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"train_loss\",\n",
    "        filename=\"choreo2groove-{epoch:02d}-{train_loss:.3f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d3af9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", version=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2b009",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "145a0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "        \"max_epochs\": epochs,\n",
    "        \"callbacks\": [checkpoint_callback],\n",
    "        \"logger\": logger,\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"check_val_every_n_epoch\": 1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "309a0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training enabled with mixed precision\n"
     ]
    }
   ],
   "source": [
    "if has_gpu:\n",
    "        trainer_kwargs.update(\n",
    "            {\n",
    "                \"accelerator\": \"gpu\",\n",
    "                \"devices\": min(gpu_count, 1),  # Use 1 GPU for now\n",
    "                \"precision\": \"16-mixed\",  # Mixed precision for faster training\n",
    "            }\n",
    "        )\n",
    "        print(\"GPU training enabled with mixed precision\")\n",
    "else:\n",
    "    trainer_kwargs.update(\n",
    "        {\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"devices\": 1,\n",
    "        }\n",
    "    )\n",
    "    print(\"CPU training mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d323284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a04144b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 30 epochs...\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bedc1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\hajin\\miniconda3\\envs\\cse-153-assignment2\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory lightning_logs\\lightning_logs\\version_0\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | PoseEncoder      | 1.1 M  | train\n",
      "1 | decoder | DrumDecoder      | 4.5 M  | train\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.474    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\hajin\\miniconda3\\envs\\cse-153-assignment2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 10/10 [00:01<00:00,  7.66it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 10/10 [00:01<00:00,  6.15it/s, v_num=0]\n",
      "\n",
      "Training completed\n",
      "Training duration: 0:00:53.356999\n",
      "Final training loss: 0.6807942390441895\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "print(f\"\\nTraining completed\")\n",
    "print(f\"Training duration: {training_duration}\")\n",
    "\n",
    "# Get final metrics\n",
    "final_loss = trainer.callback_metrics.get(\"train_loss\", \"unknown\")\n",
    "print(f\"Final training loss: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24ac5e",
   "metadata": {},
   "source": [
    "# Generate Drum Beats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc53544",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "469caddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e51cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Load the trained model with GPU support\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model = Choreo2GrooveModel(in_feats=102, lr=1e-4)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a7e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_pitch(token_name):\n",
    "    \"\"\"Convert token name back to MIDI pitch\"\"\"\n",
    "    pitch_map = {\n",
    "        \"kick\": 36,\n",
    "        \"snare\": 38,\n",
    "        \"hihat_closed\": 42,\n",
    "        \"hihat_open\": 46,\n",
    "        \"tom_low\": 45,\n",
    "        \"tom_mid\": 47,\n",
    "        \"tom_high\": 50,\n",
    "        \"crash\": 49,\n",
    "        \"ride\": 51,\n",
    "    }\n",
    "    return pitch_map.get(token_name, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3232e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, time_unit=SHIFT_SEC, bpm=120):\n",
    "    \"\"\"Convert drum tokens back to MIDI\"\"\"\n",
    "    midi = pm.PrettyMIDI(initial_tempo=bpm)\n",
    "    drums = pm.Instrument(program=0, is_drum=True, name=\"Generated_Drums\")\n",
    "\n",
    "    current_time = 0.0\n",
    "\n",
    "    for token_id in tokens:\n",
    "        if token_id >= VOCAB_SIZE:\n",
    "            continue\n",
    "\n",
    "        token_name = IDX2TOKEN.get(token_id, \"unknown\")\n",
    "        if token_name in (\"pad\", \"bos\", \"eos\"):  # <<< skip BOS/EOS\n",
    "            continue\n",
    "        elif token_name.startswith(\"shift_\"):\n",
    "            shift_amount = int(token_name.split(\"_\")[1])\n",
    "            current_time += shift_amount * time_unit\n",
    "        elif token_name in [\n",
    "            \"kick\",\n",
    "            \"snare\",\n",
    "            \"hihat_closed\",\n",
    "            \"hihat_open\",\n",
    "            \"tom_low\",\n",
    "            \"tom_mid\",\n",
    "            \"tom_high\",\n",
    "            \"crash\",\n",
    "            \"ride\",\n",
    "        ]:\n",
    "            pitch = token_to_pitch(token_name)\n",
    "            velocity = random.randint(80, 120)\n",
    "            note = pm.Note(pitch, velocity, current_time, current_time + 0.1)\n",
    "            drums.notes.append(note)\n",
    "\n",
    "    midi.instruments.append(drums)\n",
    "    return midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52acb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drum_beat(model, pose_data, max_length=256):\n",
    "    device = next(model.parameters()).device\n",
    "    pose_tensor = torch.from_numpy(pose_data).unsqueeze(0).to(device)\n",
    "\n",
    "    memory = model.encoder(pose_tensor)\n",
    "    pose_dur = pose_data.shape[0] * SHIFT_SEC  # duration\n",
    "\n",
    "    seq, elapsed = [BOS_IDX], 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            cur = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            logits = model.decoder(cur, memory)[0, -1]\n",
    "            nxt = torch.multinomial(torch.softmax(logits / 0.8, -1), 1).item()\n",
    "            seq.append(nxt)\n",
    "\n",
    "            if IDX2TOKEN[nxt].startswith(\"shift_\"):\n",
    "                elapsed += int(IDX2TOKEN[nxt].split(\"_\")[1]) * SHIFT_SEC\n",
    "            if nxt == EOS_IDX or elapsed >= pose_dur:\n",
    "                break\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd51ee",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5b753ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c266eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = \"lightning_logs/lightning_logs/version_0/checkpoints/last.ckpt\"\n",
    "output_dir = Path(\n",
    "    f\"lightning_logs/lightning_logs/version_{version}/complete_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30223ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: lightning_logs\\lightning_logs\\version_0\\complete_output\n"
     ]
    }
   ],
   "source": [
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "023d8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model(latest_checkpoint)\n",
    "dataset = ChoreoGrooveDataset(\"dataset_root\", seq_len=256)\n",
    "sample_idx = 58 # arbitrary test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c08afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_data, original_tokens = dataset[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97159204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample metadata\n",
    "metadata_path = Path(f\"dataset_root/sample_{sample_idx:03d}/metadata.txt\")\n",
    "sample_info = {}\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        for line in f:\n",
    "            if \":\" in line:\n",
    "                key, value = line.strip().split(\":\", 1)\n",
    "                sample_info[key] = value.strip()\n",
    "    # Copy metadata\n",
    "    shutil.copy(metadata_path, output_dir / \"dance_metadata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8b701",
   "metadata": {},
   "source": [
    "### Generate drum beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebe486d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = generate_drum_beat(model, pose_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b653b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_midi = tokens_to_midi(generated_tokens)\n",
    "original_midi = tokens_to_midi(original_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2b50c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_midi.write(str(output_dir / \"generated_drums.mid\"))\n",
    "original_midi.write(str(output_dir / \"original_drums.mid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd8876",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9d01635",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a104368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pose_visualization(pose_data, output_path, fps=30):\n",
    "    \"\"\"Create a video visualization of the pose data\"\"\"\n",
    "    print(\"Creating dance visualization...\")\n",
    "\n",
    "    # COCO-17 skeleton connections\n",
    "    skeleton = [\n",
    "        [15, 13],\n",
    "        [13, 11],\n",
    "        [16, 14],\n",
    "        [14, 12],\n",
    "        [11, 12],  # head\n",
    "        [5, 11],\n",
    "        [6, 12],\n",
    "        [5, 6],  # torso\n",
    "        [5, 7],\n",
    "        [6, 8],\n",
    "        [7, 9],\n",
    "        [8, 10],  # arms\n",
    "        [11, 13],\n",
    "        [12, 14],\n",
    "        [13, 15],\n",
    "        [14, 16],  # legs\n",
    "    ]\n",
    "\n",
    "    # Normalize pose data to [-1, 1] range\n",
    "    pose_flat = pose_data.reshape(len(pose_data), -1, 3)\n",
    "\n",
    "    # Calculate bounds for normalization\n",
    "    all_coords = pose_flat.reshape(-1, 3)\n",
    "    x_min, x_max = np.percentile(all_coords[:, 0], [5, 95])\n",
    "    y_min, y_max = np.percentile(all_coords[:, 1], [5, 95])\n",
    "\n",
    "    # Normalize coordinates\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    scale = max(x_max - x_min, y_max - y_min) / 1.8\n",
    "\n",
    "    pose_normalized = pose_flat.copy()\n",
    "    pose_normalized[:, :, 0] = (pose_normalized[:, :, 0] - x_center) / scale\n",
    "    pose_normalized[:, :, 1] = (pose_normalized[:, :, 1] - y_center) / scale\n",
    "\n",
    "    # Subsample frames for reasonable video length\n",
    "    stride = max(1, len(pose_normalized) // (fps * 10))\n",
    "    pose_frames = pose_normalized[::stride]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_facecolor(\"black\")\n",
    "\n",
    "    def animate(frame_idx):\n",
    "        ax.clear()\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_title(\n",
    "            f\"Choreo2Groove - Dance Visualization (Frame {frame_idx+1}/{len(pose_frames)})\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            color=\"white\",\n",
    "        )\n",
    "        ax.set_facecolor(\"black\")\n",
    "\n",
    "        if frame_idx < len(pose_frames):\n",
    "            frame = pose_frames[frame_idx]\n",
    "\n",
    "            # Draw skeleton connections\n",
    "            for connection in skeleton:\n",
    "                if connection[0] < len(frame) and connection[1] < len(frame):\n",
    "                    x_coords = [frame[connection[0]][0], frame[connection[1]][0]]\n",
    "                    y_coords = [frame[connection[0]][1], frame[connection[1]][1]]\n",
    "                    ax.plot(x_coords, y_coords, \"c-\", linewidth=2, alpha=0.8)\n",
    "\n",
    "            # Draw joints\n",
    "            for i, joint in enumerate(frame):\n",
    "                color = \"red\" if i in [0, 1, 2, 3, 4] else \"yellow\"\n",
    "                ax.scatter(joint[0], joint[1], c=color, s=50, alpha=0.9)\n",
    "\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            \"Generated by Choreo2Groove AI\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            color=\"lime\",\n",
    "            weight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.02,\n",
    "            f\"Dance Style: Basic Moves | Duration: {len(pose_data)/60:.1f}s\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=8,\n",
    "            color=\"white\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        frames=len(pose_frames),\n",
    "        interval=1000 // fps,\n",
    "        blit=False,\n",
    "        repeat=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        anim.save(str(output_path), writer=\"pillow\", fps=fps)\n",
    "        print(f\"Dance video created: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Video creation failed: {e}\")\n",
    "        gif_path = output_path.with_suffix(\".gif\")\n",
    "        anim.save(str(gif_path), writer=\"pillow\", fps=fps // 2)\n",
    "        print(f\"Created GIF instead: {gif_path}\")\n",
    "        return gif_path\n",
    "\n",
    "    plt.close(fig)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab84a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dance visualization...\n",
      "Dance video created: lightning_logs\\lightning_logs\\version_0\\complete_output\\pose.gif\n"
     ]
    }
   ],
   "source": [
    "raw_pose_path = Path(f\"dataset_root/sample_{sample_idx:03d}/pose.npy\")\n",
    "raw_pose_data = np.load(raw_pose_path)\n",
    "video_path = create_pose_visualization(\n",
    "    raw_pose_data, output_dir / \"pose.gif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf8109",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdd205",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffc2ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_movement_energy(pose_data, window_size=5):\n",
    "    \"\"\"Calculate movement energy over time from pose data\"\"\"\n",
    "    if len(pose_data.shape) == 3:\n",
    "        pose_data = pose_data.reshape(pose_data.shape[0], -1)\n",
    "\n",
    "    velocities = np.diff(pose_data, axis=0)\n",
    "    energy = np.sqrt(np.sum(velocities**2, axis=1))\n",
    "\n",
    "    if window_size > 1:\n",
    "        kernel = np.ones(window_size) / window_size\n",
    "        energy = np.convolve(energy, kernel, mode=\"same\")\n",
    "\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4cea77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_drum_timing(tokens, time_unit=0.02):\n",
    "    \"\"\"Extract drum hit timings and types from token sequence\"\"\"\n",
    "    drum_events = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for token_id in tokens:\n",
    "        if token_id >= VOCAB_SIZE:\n",
    "            continue\n",
    "\n",
    "        token_name = IDX2TOKEN.get(token_id, \"unknown\")\n",
    "\n",
    "        if token_name.startswith(\"shift_\"):\n",
    "            shift_amount = int(token_name.split(\"_\")[1])\n",
    "            current_time += shift_amount * time_unit\n",
    "        elif token_name in [\n",
    "            \"kick\",\n",
    "            \"snare\",\n",
    "            \"hihat_closed\",\n",
    "            \"hihat_open\",\n",
    "            \"tom_low\",\n",
    "            \"tom_mid\",\n",
    "            \"tom_high\",\n",
    "            \"crash\",\n",
    "            \"ride\",\n",
    "        ]:\n",
    "            drum_events.append(\n",
    "                {\n",
    "                    \"time\": current_time,\n",
    "                    \"type\": token_name,\n",
    "                    \"is_kick\": token_name == \"kick\",\n",
    "                    \"is_snare\": token_name == \"snare\",\n",
    "                    \"is_accent\": token_name in [\"kick\", \"snare\", \"crash\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return drum_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0a30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_movement_beat_correlation(pose_data, drum_events, fps=60):\n",
    "    \"\"\"Calculate correlation between movement energy and drum beats\"\"\"\n",
    "    energy = calculate_movement_energy(pose_data)\n",
    "    pose_times = np.arange(len(energy)) / fps\n",
    "    max_time = pose_times[-1] if len(pose_times) > 0 else 10.0\n",
    "    drum_timeline = np.zeros(len(pose_times))\n",
    "\n",
    "    for event in drum_events:\n",
    "        if event[\"time\"] <= max_time:\n",
    "            frame_idx = int(event[\"time\"] * fps)\n",
    "            if frame_idx < len(drum_timeline):\n",
    "                weight = 3.0 if event[\"is_accent\"] else 1.0\n",
    "                drum_timeline[frame_idx] += weight\n",
    "\n",
    "    min_len = min(len(energy), len(drum_timeline))\n",
    "    if min_len > 10:\n",
    "        correlation = np.corrcoef(energy[:min_len], drum_timeline[:min_len])[0, 1]\n",
    "    else:\n",
    "        correlation = 0.0\n",
    "\n",
    "    return correlation, energy, drum_timeline, pose_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b5fb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignment_visualization(\n",
    "    times,\n",
    "    energy,\n",
    "    gen_drums,\n",
    "    orig_drums,\n",
    "    gen_events,\n",
    "    orig_events,\n",
    "    analysis,\n",
    "    sample_idx,\n",
    "    output_dir,\n",
    "):\n",
    "    \"\"\"Create comprehensive visualization of movement-beat alignment\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    fig.suptitle(\n",
    "        f\"Choreo2Groove Alignment Analysis - Sample {sample_idx}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 1. Movement Energy\n",
    "    axes[0].plot(\n",
    "        times[: len(energy)], energy, \"b-\", linewidth=2, label=\"Movement Energy\"\n",
    "    )\n",
    "    axes[0].set_title(\"Dance Movement Energy Over Time\", fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Energy\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # 2. Generated Drums with Movement\n",
    "    axes[1].plot(times[: len(energy)], energy, \"b-\", alpha=0.5, label=\"Movement Energy\")\n",
    "    axes[1].bar(\n",
    "        times[: len(gen_drums)],\n",
    "        gen_drums[: len(times)],\n",
    "        alpha=0.7,\n",
    "        color=\"red\",\n",
    "        width=0.01,\n",
    "        label=\"AI Generated Drums\",\n",
    "    )\n",
    "\n",
    "    for event in gen_events:\n",
    "        if event[\"time\"] <= times[-1]:\n",
    "            color = (\n",
    "                \"darkred\"\n",
    "                if event[\"is_kick\"]\n",
    "                else \"orange\" if event[\"is_snare\"] else \"pink\"\n",
    "            )\n",
    "            axes[1].axvline(x=event[\"time\"], color=color, alpha=0.8, linewidth=2)\n",
    "\n",
    "    axes[1].set_title(\n",
    "        f\"AI Generated Drums vs Movement (Correlation: {analysis['generated_analysis']['movement_correlation']:.3f})\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Intensity\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Original Drums with Movement\n",
    "    axes[2].plot(times[: len(energy)], energy, \"b-\", alpha=0.5, label=\"Movement Energy\")\n",
    "    axes[2].bar(\n",
    "        times[: len(orig_drums)],\n",
    "        orig_drums[: len(times)],\n",
    "        alpha=0.7,\n",
    "        color=\"green\",\n",
    "        width=0.01,\n",
    "        label=\"Original Drums\",\n",
    "    )\n",
    "\n",
    "    for event in orig_events:\n",
    "        if event[\"time\"] <= times[-1]:\n",
    "            color = (\n",
    "                \"darkgreen\"\n",
    "                if event[\"is_kick\"]\n",
    "                else \"lightgreen\" if event[\"is_snare\"] else \"lime\"\n",
    "            )\n",
    "            axes[2].axvline(x=event[\"time\"], color=color, alpha=0.8, linewidth=2)\n",
    "\n",
    "    axes[2].set_title(\n",
    "        f\"Original Drums vs Movement (Correlation: {analysis['original_analysis']['movement_correlation']:.3f})\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[2].set_ylabel(\"Intensity\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Comparison Bar Chart\n",
    "    categories = [\"Drum Events\", \"Kick Hits\", \"Snare Hits\", \"Total Accents\"]\n",
    "    generated_values = [\n",
    "        analysis[\"generated_analysis\"][\"drum_events\"],\n",
    "        analysis[\"generated_analysis\"][\"kick_events\"],\n",
    "        analysis[\"generated_analysis\"][\"snare_events\"],\n",
    "        analysis[\"generated_analysis\"][\"total_accents\"],\n",
    "    ]\n",
    "    original_values = [\n",
    "        analysis[\"original_analysis\"][\"drum_events\"],\n",
    "        analysis[\"original_analysis\"][\"kick_events\"],\n",
    "        analysis[\"original_analysis\"][\"snare_events\"],\n",
    "        analysis[\"original_analysis\"][\"total_accents\"],\n",
    "    ]\n",
    "\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[3].bar(\n",
    "        x - width / 2,\n",
    "        generated_values,\n",
    "        width,\n",
    "        label=\"AI Generated\",\n",
    "        color=\"red\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[3].bar(\n",
    "        x + width / 2,\n",
    "        original_values,\n",
    "        width,\n",
    "        label=\"Original\",\n",
    "        color=\"green\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    axes[3].set_title(\"Drum Pattern Comparison\", fontweight=\"bold\")\n",
    "    axes[3].set_ylabel(\"Count\")\n",
    "    axes[3].set_xticks(x)\n",
    "    axes[3].set_xticklabels(categories)\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = output_dir / f\"alignment_analysis_sample_{sample_idx}.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29a29a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_drums = extract_drum_timing(generated_tokens)\n",
    "original_drums = extract_drum_timing(original_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ebc1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_corr, gen_energy, gen_timeline, times = calculate_movement_beat_correlation(\n",
    "        pose_data.numpy(), generated_drums\n",
    "    )\n",
    "orig_corr, _, orig_timeline, _ = calculate_movement_beat_correlation(\n",
    "    pose_data.numpy(), original_drums\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "912d068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {\n",
    "    \"sample_info\": {\n",
    "        \"sample_id\": sample_idx,\n",
    "        \"pose_frames\": len(pose_data),\n",
    "        \"duration_seconds\": len(pose_data) / 60.0,\n",
    "    },\n",
    "    \"generated_analysis\": {\n",
    "        \"drum_events\": len(generated_drums),\n",
    "        \"movement_correlation\": float(gen_corr) if not np.isnan(gen_corr) else 0.0,\n",
    "        \"kick_events\": sum(1 for d in generated_drums if d[\"is_kick\"]),\n",
    "        \"snare_events\": sum(1 for d in generated_drums if d[\"is_snare\"]),\n",
    "        \"total_accents\": sum(1 for d in generated_drums if d[\"is_accent\"]),\n",
    "    },\n",
    "    \"original_analysis\": {\n",
    "        \"drum_events\": len(original_drums),\n",
    "        \"movement_correlation\": (\n",
    "            float(orig_corr) if not np.isnan(orig_corr) else 0.0\n",
    "        ),\n",
    "        \"kick_events\": sum(1 for d in original_drums if d[\"is_kick\"]),\n",
    "        \"snare_events\": sum(1 for d in original_drums if d[\"is_snare\"]),\n",
    "        \"total_accents\": sum(1 for d in original_drums if d[\"is_accent\"]),\n",
    "    },\n",
    "    \"alignment_quality\": {\n",
    "        \"generated_vs_movement\": (\n",
    "            \"Good\" if gen_corr > 0.2 else \"Moderate\" if gen_corr > 0.1 else \"Weak\"\n",
    "        ),\n",
    "        \"compared_to_original\": (\n",
    "            \"Better\"\n",
    "            if gen_corr > orig_corr\n",
    "            else \"Similar\" if abs(gen_corr - orig_corr) < 0.05 else \"Worse\"\n",
    "        ),\n",
    "        \"correlation_difference\": (\n",
    "            float(gen_corr - orig_corr)\n",
    "            if not np.isnan(gen_corr - orig_corr)\n",
    "            else 0.0\n",
    "        ),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "620e4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image at: lightning_logs\\lightning_logs\\version_0\\complete_output\\alignment_analysis_sample_58.png\n"
     ]
    }
   ],
   "source": [
    "plot_path = create_alignment_visualization(\n",
    "    times,\n",
    "    gen_energy,\n",
    "    gen_timeline,\n",
    "    orig_timeline,\n",
    "    generated_drums,\n",
    "    original_drums,\n",
    "    analysis,\n",
    "    sample_idx,\n",
    "    output_dir,\n",
    ")\n",
    "\n",
    "print(f\"Image at: {plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-153-assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
