{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca02b63b",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Conditioned drum beat prediction on 2-d pose data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272af5b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c1399df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q numpy pretty_midi torch pytorch_lightning matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2411934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab41c",
   "metadata": {},
   "source": [
    "### Model training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd553e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT_SEC = 0.02\n",
    "DRUM_TOKENS: Dict[str, int] = {\n",
    "    \"pad\": 0,\n",
    "    \"kick\": 1,\n",
    "    \"snare\": 2,\n",
    "    \"hihat_closed\": 3,\n",
    "    \"hihat_open\": 4,\n",
    "    \"tom_low\": 5,\n",
    "    \"tom_mid\": 6,\n",
    "    \"tom_high\": 7,\n",
    "    \"crash\": 8,\n",
    "    \"ride\": 9,\n",
    "}\n",
    "# time‑shift tokens (20 ms each, up to 2 s)\n",
    "SHIFT_OFFSET = len(DRUM_TOKENS)\n",
    "MAX_SHIFT = 100  # 100 × 20 ms  = 2 s\n",
    "for i in range(1, MAX_SHIFT + 1):\n",
    "    DRUM_TOKENS[f\"shift_{i}\"] = SHIFT_OFFSET + i\n",
    "\n",
    "# sequence control tokens – **added**\n",
    "DRUM_TOKENS[\"bos\"] = len(DRUM_TOKENS)  # begin‑of‑sequence\n",
    "DRUM_TOKENS[\"eos\"] = len(DRUM_TOKENS)  # end‑of‑sequence\n",
    "\n",
    "VOCAB_SIZE = len(DRUM_TOKENS)\n",
    "IDX2TOKEN = {v: k for k, v in DRUM_TOKENS.items()}\n",
    "PAD_IDX = DRUM_TOKENS[\"pad\"]\n",
    "BOS_IDX = DRUM_TOKENS[\"bos\"]\n",
    "EOS_IDX = DRUM_TOKENS[\"eos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d3a67c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "715e78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pitch_to_token(p: int) -> str:\n",
    "    # General MIDI → symbolic token\n",
    "    return (\n",
    "        \"kick\"\n",
    "        if p in (35, 36)\n",
    "        else (\n",
    "            \"snare\"\n",
    "            if p in (38, 40)\n",
    "            else (\n",
    "                \"hihat_closed\"\n",
    "                if p in (42, 44)\n",
    "                else (\n",
    "                    \"hihat_open\"\n",
    "                    if p == 46\n",
    "                    else (\n",
    "                        \"tom_low\"\n",
    "                        if p in (41, 45)\n",
    "                        else (\n",
    "                            \"tom_mid\"\n",
    "                            if p in (47, 48)\n",
    "                            else (\n",
    "                                \"tom_high\"\n",
    "                                if p == 50\n",
    "                                else (\n",
    "                                    \"crash\"\n",
    "                                    if p in (49, 57)\n",
    "                                    else \"ride\" if p in (51, 59) else \"snare\"\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f47fc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_tokens(mid: pm.PrettyMIDI, time_unit: float = SHIFT_SEC) -> List[int]:\n",
    "    \"\"\"Drum MIDI → event tokens (no BOS/EOS).\"\"\"\n",
    "    events: List[Tuple[float, str]] = []\n",
    "    for inst in mid.instruments:\n",
    "        if not inst.is_drum:\n",
    "            continue\n",
    "        for note in inst.notes:\n",
    "            events.append((note.start, _pitch_to_token(note.pitch)))\n",
    "    events.sort(key=lambda x: x[0])\n",
    "\n",
    "    tokens, prev_time = [], 0.0\n",
    "    for t, tok in events:\n",
    "        delta = t - prev_time\n",
    "        n_shift = int(round(delta / time_unit))\n",
    "        while n_shift > MAX_SHIFT:\n",
    "            tokens.append(DRUM_TOKENS[f\"shift_{MAX_SHIFT}\"])\n",
    "            n_shift -= MAX_SHIFT\n",
    "        if n_shift > 0:\n",
    "            tokens.append(DRUM_TOKENS[f\"shift_{n_shift}\"])\n",
    "        tokens.append(DRUM_TOKENS[tok])\n",
    "        prev_time = t\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c221fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pose, tok = zip(*batch)\n",
    "    return (torch.nn.utils.rnn.pad_sequence(pose, batch_first=True), torch.stack(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21b319",
   "metadata": {},
   "source": [
    "### Model dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b780764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoreoGrooveDataset(Dataset):\n",
    "    def __init__(self, root: str, seq_len: int = 512):\n",
    "        self.items = sorted(glob(os.path.join(root, \"*\", \"pose.npy\")))\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pose_path = self.items[idx]\n",
    "        drum_path = pose_path.replace(\"pose.npy\", \"drums.mid\")\n",
    "\n",
    "        # pose → features\n",
    "        pose = np.load(pose_path).reshape(-1, 51)  # (T, 17×3)\n",
    "        vel = np.diff(pose, axis=0, prepend=pose[:1])\n",
    "        feats = np.concatenate([pose, vel], axis=-1)  # (T, 102)\n",
    "        feats = (feats - feats.mean()) / (feats.std() + 1e-5)\n",
    "        feats = feats.astype(np.float32)\n",
    "\n",
    "        # drums → tokens  [+ BOS/EOS, pad/trim]\n",
    "        tokens = [BOS_IDX] + midi_to_tokens(pm.PrettyMIDI(drum_path)) + [EOS_IDX]\n",
    "        if len(tokens) < self.seq_len:\n",
    "            tokens += [PAD_IDX] * (self.seq_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[: self.seq_len]\n",
    "\n",
    "        return torch.from_numpy(feats), torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7411",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9f2868be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEncoder(nn.Module):\n",
    "    def __init__(self, in_feats=102, embed=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_feats, 128, 5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, embed, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRU(embed, embed, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(embed * 2, embed)\n",
    "\n",
    "    def forward(self, x):  # x (B,T,F)\n",
    "        x = self.conv(x.transpose(1, 2)).transpose(1, 2)  # (B,T,E)\n",
    "        x, _ = self.gru(x)\n",
    "        return self.proj(x).transpose(0, 1)  # (T,B,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9234955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumDecoder(nn.Module):\n",
    "    def __init__(self, embed=256, layers=4, nhead=8, vocab=VOCAB_SIZE):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab, embed)\n",
    "        self.pos_emb = nn.Embedding(1024, embed)\n",
    "        dec_layer = nn.TransformerDecoderLayer(embed, nhead, 1024, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(dec_layer, layers)\n",
    "        self.fc_out = nn.Linear(embed, vocab)\n",
    "\n",
    "    def forward(self, tgt, memory):  # tgt (B,L), memory (T,B,E)\n",
    "        pos = torch.arange(tgt.size(1), device=tgt.device).unsqueeze(0)\n",
    "        tgt = self.tok_emb(tgt) + self.pos_emb(pos)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(\n",
    "            tgt.device\n",
    "        )\n",
    "        out = self.transformer(tgt, memory.transpose(0, 1), tgt_mask=mask)\n",
    "        return self.fc_out(out)  # (B,L,V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d87da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Choreo2GrooveModel(pl.LightningModule):\n",
    "    def __init__(self, in_feats: int, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.encoder = PoseEncoder(in_feats)\n",
    "        self.decoder = DrumDecoder()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    # forward\n",
    "    def forward(self, poses, tokens):\n",
    "        memory = self.encoder(poses)  # (T,B,E)\n",
    "        if self.training:\n",
    "            tgt_in = tokens[:, :-1]  # strip last (EOS / PAD)\n",
    "            return self.decoder(tgt_in, memory)  # (B,L‑1,V)\n",
    "        else:\n",
    "            return self.decoder(tokens, memory)\n",
    "\n",
    "    # training\n",
    "    def training_step(self, batch, _):\n",
    "        pose, tok = batch\n",
    "        logits = self(pose, tok)\n",
    "        loss = self.loss_fn(logits.reshape(-1, VOCAB_SIZE), tok[:, 1:].reshape(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67b722",
   "metadata": {},
   "source": [
    "### Training Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "62636b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "lr=1e-4\n",
    "batch_size=4\n",
    "seq_len=256\n",
    "version=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8491ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_availability():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"Using GPU\")\n",
    "        return True, gpu_count\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4febe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "has_gpu, gpu_count = check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cef359a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DataLoader - optimized for GPU\n",
    "num_workers = (\n",
    "    0 if sys.platform.startswith(\"win\") else min(4, gpu_count * 2) if has_gpu else 2\n",
    ")\n",
    "pin_memory = has_gpu  # Use pinned memory for faster GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c99dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: increasing batch size from 4 to 8\n"
     ]
    }
   ],
   "source": [
    "# Adjust batch size for GPU if available\n",
    "if has_gpu and batch_size < 8:\n",
    "    original_batch_size = batch_size\n",
    "    batch_size = min(16, batch_size * 2)  # Increase batch size for GPU\n",
    "    print(\n",
    "        f\"GPU detected: increasing batch size from {original_batch_size} to {batch_size}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0d477",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eca01b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChoreoGrooveDataset(\"dataset_root\", seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58f57692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 76 samples, 102 features per frame\n"
     ]
    }
   ],
   "source": [
    "# Calculate input features from first sample\n",
    "sample_pose, _ = dataset[0]\n",
    "in_feats = sample_pose.shape[-1]\n",
    "print(f\"Dataset loaded: {len(dataset)} samples, {in_feats} features per frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c5a1e",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0d79073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Choreo2GrooveModel(in_feats=in_feats, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e10ab1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=num_workers > 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184a6f6",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffaaf9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"train_loss\",\n",
    "        filename=\"choreo2groove-{epoch:02d}-{train_loss:.3f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d3af9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", version=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2b009",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "145a0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "        \"max_epochs\": epochs,\n",
    "        \"callbacks\": [checkpoint_callback],\n",
    "        \"logger\": logger,\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"check_val_every_n_epoch\": 1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "309a0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training enabled with mixed precision\n"
     ]
    }
   ],
   "source": [
    "if has_gpu:\n",
    "        trainer_kwargs.update(\n",
    "            {\n",
    "                \"accelerator\": \"gpu\",\n",
    "                \"devices\": min(gpu_count, 1),  # Use 1 GPU for now\n",
    "                \"precision\": \"16-mixed\",  # Mixed precision for faster training\n",
    "            }\n",
    "        )\n",
    "        print(\"GPU training enabled with mixed precision\")\n",
    "else:\n",
    "    trainer_kwargs.update(\n",
    "        {\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"devices\": 1,\n",
    "        }\n",
    "    )\n",
    "    print(\"CPU training mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3d323284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**trainer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a04144b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bedc1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hajin\\miniconda3\\envs\\cse-153-assignment2\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory lightning_logs\\lightning_logs\\version_0\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | PoseEncoder      | 1.1 M  | train\n",
      "1 | decoder | DrumDecoder      | 4.5 M  | train\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.474    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\hajin\\miniconda3\\envs\\cse-153-assignment2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:01<00:00,  7.63it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:01<00:00,  5.08it/s, v_num=0]\n",
      "\n",
      "Training completed\n",
      "Training duration: 0:00:10.149039\n",
      "Final training loss: 0.8090565204620361\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "print(f\"\\nTraining completed\")\n",
    "print(f\"Training duration: {training_duration}\")\n",
    "\n",
    "# Get final metrics\n",
    "final_loss = trainer.callback_metrics.get(\"train_loss\", \"unknown\")\n",
    "print(f\"Final training loss: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24ac5e",
   "metadata": {},
   "source": [
    "# Generate Drum Beats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc53544",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "469caddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8e51cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Load the trained model with GPU support\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model = Choreo2GrooveModel(in_feats=102, lr=1e-4)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a7e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_pitch(token_name):\n",
    "    \"\"\"Convert token name back to MIDI pitch\"\"\"\n",
    "    pitch_map = {\n",
    "        \"kick\": 36,\n",
    "        \"snare\": 38,\n",
    "        \"hihat_closed\": 42,\n",
    "        \"hihat_open\": 46,\n",
    "        \"tom_low\": 45,\n",
    "        \"tom_mid\": 47,\n",
    "        \"tom_high\": 50,\n",
    "        \"crash\": 49,\n",
    "        \"ride\": 51,\n",
    "    }\n",
    "    return pitch_map.get(token_name, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_midi(tokens, time_unit=SHIFT_SEC, bpm=120):\n",
    "    \"\"\"Convert drum tokens back to MIDI\"\"\"\n",
    "    midi = pm.PrettyMIDI(initial_tempo=bpm)\n",
    "    drums = pm.Instrument(program=0, is_drum=True, name=\"Generated_Drums\")\n",
    "\n",
    "    current_time = 0.0\n",
    "\n",
    "    for token_id in tokens:\n",
    "        if token_id >= VOCAB_SIZE:\n",
    "            continue\n",
    "\n",
    "        token_name = IDX2TOKEN.get(token_id, \"unknown\")\n",
    "        if token_name in (\"pad\", \"bos\", \"eos\"):  # <<< skip BOS/EOS\n",
    "            continue\n",
    "        elif token_name.startswith(\"shift_\"):\n",
    "            shift_amount = int(token_name.split(\"_\")[1])\n",
    "            current_time += shift_amount * time_unit\n",
    "        elif token_name in [\n",
    "            \"kick\",\n",
    "            \"snare\",\n",
    "            \"hihat_closed\",\n",
    "            \"hihat_open\",\n",
    "            \"tom_low\",\n",
    "            \"tom_mid\",\n",
    "            \"tom_high\",\n",
    "            \"crash\",\n",
    "            \"ride\",\n",
    "        ]:\n",
    "            pitch = token_to_pitch(token_name)\n",
    "            velocity = random.randint(80, 120)\n",
    "            note = pm.Note(pitch, velocity, current_time, current_time + 0.1)\n",
    "            drums.notes.append(note)\n",
    "\n",
    "    midi.instruments.append(drums)\n",
    "    return midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_drum_beat(model, pose_data, max_length=256):\n",
    "    device = next(model.parameters()).device\n",
    "    pose_tensor = torch.from_numpy(pose_data).unsqueeze(0).to(device)\n",
    "\n",
    "    memory = model.encoder(pose_tensor)\n",
    "    pose_dur = pose_data.shape[0] * SHIFT_SEC  # duration\n",
    "\n",
    "    seq, elapsed = [BOS_IDX], 0.0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            cur = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            logits = model.decoder(cur, memory)[0, -1]\n",
    "            nxt = torch.multinomial(torch.softmax(logits / 0.8, -1), 1).item()\n",
    "            seq.append(nxt)\n",
    "\n",
    "            if IDX2TOKEN[nxt].startswith(\"shift_\"):\n",
    "                elapsed += int(IDX2TOKEN[nxt].split(\"_\")[1]) * SHIFT_SEC\n",
    "            if nxt == EOS_IDX or elapsed >= pose_dur:\n",
    "                break\n",
    "    return seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-153-assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
